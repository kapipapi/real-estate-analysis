{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <761A4B43-4CD1-322C-BB16-CEE783FE0A7C> /opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 30805/3685/OMS\n",
      "Skipping 22508/3685/OMS\n",
      "Skipping 29787/3685/OMS\n",
      "Skipping 27080/3685/OMS\n",
      "Skipping 27417/3685/OMS\n",
      "Skipping 30832/3685/OMS\n",
      "Skipping 27285/3685/OMS\n",
      "Skipping 27282/3685/OMS\n",
      "Skipping 22101/3685/OMS\n",
      "Skipping 25924/3685/OMS\n",
      "Skipping 29208/3685/OMS\n",
      "Skipping 24977/3685/OMS\n",
      "Skipping 31096/3685/OMS\n",
      "Skipping 28987/3685/OMS\n",
      "Skipping 24127/3685/OMS\n",
      "Skipping 30778/3685/OMS\n",
      "Skipping 26877/3685/OMS\n",
      "Skipping 29807/3685/OMS\n",
      "Skipping 31400/3685/OMS\n",
      "Skipping 22474/3685/OMS\n",
      "Skipping 29044/3685/OMS\n",
      "Skipping 25484/3685/OMS\n",
      "Skipping 29623/3685/OMS\n",
      "Skipping 22831/3685/OMS\n",
      "Skipping 31211/3685/OMS\n",
      "Skipping 28767/3685/OMS\n",
      "Skipping 24502/3685/OMS\n",
      "Skipping 29614/3685/OMS\n",
      "Skipping 29045/3685/OMS\n",
      "Skipping 31288/3685/OMS\n",
      "Skipping 23973/3685/OMS\n",
      "Skipping 28902/3685/OMS\n",
      "Skipping 29358/3685/OMS\n",
      "Skipping 29361/3685/OMS\n",
      "Skipping 31399/3685/OMS\n",
      "Skipping 23238/3685/OMS\n",
      "Skipping 27495/3685/OMS\n",
      "Skipping 31133/3685/OMS\n",
      "Skipping 26308/3685/OMS\n",
      "Skipping 26592/3685/OMS\n",
      "Skipping 27440/3685/OMS\n",
      "Skipping 27827/3685/OMS\n",
      "Skipping 25577/3685/OMS\n",
      "Skipping 29144/3685/OMS\n",
      "Skipping 28694/3685/OMS\n",
      "Skipping 27280/3685/OMS\n",
      "Skipping 25147/3685/OMS\n",
      "Skipping 27226/3685/OMS\n",
      "Skipping 29145/3685/OMS\n",
      "Skipping 31344/3685/OMS\n",
      "Skipping 28090/3685/OMS\n",
      "Skipping 23471/3685/OMS\n",
      "Skipping 29127/3685/OMS\n",
      "Skipping 25389/3685/OMS\n",
      "Skipping 30831/3685/OMS\n",
      "Skipping 24123/3685/OMS\n",
      "Skipping 25806/3685/OMS\n",
      "Skipping 26618/3685/OMS\n",
      "Skipping 28111/3685/OMS\n",
      "Skipping 24780/3685/OMS\n",
      "Skipping 28516/3685/OMS\n",
      "Skipping 25664/3685/OMS\n",
      "Skipping 26218/3685/OMS\n",
      "Skipping 29031/3685/OMS\n",
      "Skipping 23159/3685/OMS\n",
      "Skipping 24981/3685/OMS\n",
      "Skipping 26603/3685/OMS\n",
      "Skipping 29219/3685/OMS\n",
      "Skipping 21279/3685/OMS\n",
      "Skipping 26695/3685/OMS\n",
      "Skipping 29243/3685/OMS\n",
      "Skipping 29244/3685/OMS\n",
      "Skipping 29076/3685/OMS\n",
      "Skipping 28566/3685/OMS\n",
      "Skipping 28798/3685/OMS\n",
      "Skipping 26868/3685/OMS\n",
      "Skipping 25197/3685/OMS\n",
      "Skipping 30683/3685/OMS\n",
      "Skipping 28471/3685/OMS\n",
      "Skipping 21357/3685/OMS\n",
      "Skipping 26529/3685/OMS\n",
      "Skipping 26784/3685/OMS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['31093/3685/OMS',\n",
       " '29821/3685/OMS',\n",
       " '26856/3685/OMS',\n",
       " '24508/3685/OMS',\n",
       " '30768/3685/OMS',\n",
       " '28195/3685/OMS',\n",
       " '31283/3685/OMS',\n",
       " '29015/3685/OMS',\n",
       " '22120/3685/OMS',\n",
       " '28221/3685/OMS',\n",
       " '31334/3685/OMS']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offers = []\n",
    "for root, _, files in os.walk(f\"./images\"):\n",
    "    if len(root.split(\"/\")) == 5:\n",
    "        offer = root.split(\"/\")[2:]\n",
    "        filename = \"_\".join(offer)\n",
    "        offer_id = \"/\".join(offer)\n",
    "\n",
    "        if not os.path.isfile(f\"./images_embeded/{filename}.pkl\"):\n",
    "            offers.append(offer_id)\n",
    "        else:\n",
    "            print(f\"Skipping {offer_id}\")\n",
    "\n",
    "offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photos(offer_id, transform=None):\n",
    "    batch = []\n",
    "    for root, _, files in os.walk(f\"./images/{offer_id}\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") and \"all\" not in file and \"wp\" not in file:\n",
    "                image_path = os.path.join(root, file)\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "                if transform:\n",
    "                    image = transform(image)\n",
    "                else:\n",
    "                    image = transforms.ToTensor()(image)\n",
    "\n",
    "                batch.append(image)\n",
    "\n",
    "    if len(batch) == 0:\n",
    "        return None\n",
    "\n",
    "    return torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "s = 14 * 16\n",
    "m = 14 * 32 \n",
    "l = 14 * 48 \n",
    "xl = 14 * 64\n",
    "\n",
    "S = (s, s)\n",
    "M = (m, m)\n",
    "L = (l, l) # good for 18GB VRAM\n",
    "XL = (xl, xl)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(L),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14_reg\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 31093/3685/OMS\n",
      "Skipping 31093/3685/OMS - no photos found\n",
      "Processing 29821/3685/OMS\n",
      "torch.Size([11, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]/opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torch/nn/functional.py:4072: UserWarning: The operator 'aten::_upsample_bicubic2d_aa.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1716905753263/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors)\n",
      "100%|██████████| 11/11 [00:01<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/29821_3685_OMS.pkl\n",
      "Processing 26856/3685/OMS\n",
      "torch.Size([26, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:02<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/26856_3685_OMS.pkl\n",
      "Processing 24508/3685/OMS\n",
      "torch.Size([27, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:02<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/24508_3685_OMS.pkl\n",
      "Processing 30768/3685/OMS\n",
      "torch.Size([14, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:01<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/30768_3685_OMS.pkl\n",
      "Processing 28195/3685/OMS\n",
      "torch.Size([19, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:02<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/28195_3685_OMS.pkl\n",
      "Processing 31283/3685/OMS\n",
      "Skipping 31283/3685/OMS - no photos found\n",
      "Processing 29015/3685/OMS\n",
      "torch.Size([7, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/29015_3685_OMS.pkl\n",
      "Processing 22120/3685/OMS\n",
      "torch.Size([8, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/22120_3685_OMS.pkl\n",
      "Processing 28221/3685/OMS\n",
      "torch.Size([20, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/28221_3685_OMS.pkl\n",
      "Processing 31334/3685/OMS\n",
      "torch.Size([9, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/31334_3685_OMS.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for offer in offers:\n",
    "    print(f\"Processing {offer}\")\n",
    "\n",
    "    images = get_photos(offer_id=offer, transform=transform)\n",
    "\n",
    "    if images is None:\n",
    "        print(f\"Skipping {offer} - no photos found\")\n",
    "        continue\n",
    "\n",
    "    print(images.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for i, image in tqdm(enumerate(images), total=len(images)):\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        output = model(image).squeeze(0).detach().cpu().numpy()\n",
    "        outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "    filename = \"_\".join(offer.split(\"/\"))\n",
    "    with open(f'./images_embeded/{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'image_size': L,\n",
    "            'offer_id': offer,\n",
    "            'outputs': outputs,\n",
    "        }, f)\n",
    "    print(f\"Saved to ./images_embeded/{filename}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_size': (672, 672),\n",
       " 'offer_id': '30805/3685/OMS',\n",
       " 'outputs': array([[ 2.560301  , -0.8504364 , -0.21759939, ...,  1.52431   ,\n",
       "          1.125218  ,  1.6827465 ],\n",
       "        [ 1.9245269 , -0.48709422,  1.9455004 , ..., -0.94634795,\n",
       "          0.5189094 ,  0.967928  ],\n",
       "        [ 1.4143376 , -1.6160402 ,  0.3519848 , ...,  2.9981334 ,\n",
       "          1.7953353 ,  2.053301  ],\n",
       "        ...,\n",
       "        [-0.58763826, -0.72419184, -0.693054  , ..., -1.3873552 ,\n",
       "          1.4864492 , -2.251434  ],\n",
       "        [ 1.7955742 , -1.2687701 , -0.7398259 , ..., -0.5495759 ,\n",
       "          1.591101  ,  0.20677093],\n",
       "        [ 1.497448  , -0.86951196,  0.15145272, ..., -0.02245994,\n",
       "          1.4777577 ,  1.5281867 ]], dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./images_embeded/30805_3685_OMS.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real-estate-dinov2-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
