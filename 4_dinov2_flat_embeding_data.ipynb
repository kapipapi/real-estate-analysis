{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <761A4B43-4CD1-322C-BB16-CEE783FE0A7C> /opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchmetrics\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['30805/3685/OMS',\n",
       " '22508/3685/OMS',\n",
       " '27080/3685/OMS',\n",
       " '30832/3685/OMS',\n",
       " '27285/3685/OMS',\n",
       " '27282/3685/OMS',\n",
       " '22101/3685/OMS',\n",
       " '25924/3685/OMS',\n",
       " '29208/3685/OMS',\n",
       " '24977/3685/OMS',\n",
       " '31096/3685/OMS',\n",
       " '28987/3685/OMS',\n",
       " '24127/3685/OMS',\n",
       " '30778/3685/OMS',\n",
       " '22474/3685/OMS',\n",
       " '29044/3685/OMS',\n",
       " '25484/3685/OMS',\n",
       " '29623/3685/OMS',\n",
       " '22831/3685/OMS',\n",
       " '31211/3685/OMS',\n",
       " '28767/3685/OMS',\n",
       " '24502/3685/OMS',\n",
       " '29614/3685/OMS',\n",
       " '29045/3685/OMS',\n",
       " '31288/3685/OMS',\n",
       " '28902/3685/OMS',\n",
       " '29358/3685/OMS',\n",
       " '29361/3685/OMS',\n",
       " '31399/3685/OMS',\n",
       " '23238/3685/OMS',\n",
       " '27495/3685/OMS',\n",
       " '31133/3685/OMS',\n",
       " '27440/3685/OMS',\n",
       " '27827/3685/OMS',\n",
       " '27280/3685/OMS',\n",
       " '25147/3685/OMS',\n",
       " '27226/3685/OMS',\n",
       " '31344/3685/OMS',\n",
       " '28090/3685/OMS',\n",
       " '23471/3685/OMS',\n",
       " '29127/3685/OMS',\n",
       " '25389/3685/OMS',\n",
       " '30831/3685/OMS',\n",
       " '24123/3685/OMS',\n",
       " '26618/3685/OMS',\n",
       " '28111/3685/OMS',\n",
       " '24780/3685/OMS',\n",
       " '28516/3685/OMS',\n",
       " '25664/3685/OMS',\n",
       " '26218/3685/OMS',\n",
       " '29031/3685/OMS',\n",
       " '23159/3685/OMS',\n",
       " '24981/3685/OMS',\n",
       " '26603/3685/OMS',\n",
       " '29219/3685/OMS',\n",
       " '21279/3685/OMS',\n",
       " '26695/3685/OMS',\n",
       " '29243/3685/OMS',\n",
       " '29244/3685/OMS',\n",
       " '29076/3685/OMS',\n",
       " '28566/3685/OMS',\n",
       " '28798/3685/OMS',\n",
       " '26868/3685/OMS',\n",
       " '25197/3685/OMS',\n",
       " '30683/3685/OMS',\n",
       " '28471/3685/OMS',\n",
       " '21357/3685/OMS',\n",
       " '26529/3685/OMS',\n",
       " '26784/3685/OMS']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offers = []\n",
    "for root, _, files in os.walk(f\"./images\"):\n",
    "    if len(root.split(\"/\")) == 5:\n",
    "        offers.append(\"/\".join(root.split(\"/\")[2:]))\n",
    "\n",
    "offers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photos(offer_id, transform=None):\n",
    "    batch = []\n",
    "    for root, _, files in os.walk(f\"./images/{offer_id}\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") and \"all\" not in file and \"wp\" not in file:\n",
    "                image_path = os.path.join(root, file)\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "                if transform:\n",
    "                    image = transform(image)\n",
    "                else:\n",
    "                    image = transforms.ToTensor()(image)\n",
    "\n",
    "                batch.append(image)\n",
    "\n",
    "    return torch.stack(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/Users/kacper/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "s = 14 * 16\n",
    "m = 14 * 32 \n",
    "l = 14 * 48 \n",
    "xl = 14 * 64\n",
    "\n",
    "S = (s, s)\n",
    "M = (m, m)\n",
    "L = (l, l) # good for 18GB VRAM\n",
    "XL = (xl, xl)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(L),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14_reg\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 30805/3685/OMS\n",
      "torch.Size([28, 3, 672, 672])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28 [00:00<?, ?it/s]/opt/miniconda3/envs/real-estate-analysis-torch/lib/python3.11/site-packages/torch/nn/functional.py:4072: UserWarning: The operator 'aten::_upsample_bicubic2d_aa.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1716905753263/work/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  return torch._C._nn._upsample_bicubic2d_aa(input, output_size, align_corners, scale_factors)\n",
      "100%|██████████| 28/28 [00:03<00:00,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./images_embeded/30805_3685_OMS.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for offer in offers:\n",
    "    print(f\"Processing {offer}\")\n",
    "\n",
    "    images = get_photos(offer_id=offer, transform=transform)\n",
    "    print(images.shape)\n",
    "\n",
    "    outputs = []\n",
    "    for i, image in tqdm(enumerate(images), total=len(images)):\n",
    "        image = image.unsqueeze(0).to(device)\n",
    "        output = model(image).squeeze(0).detach().cpu().numpy()\n",
    "        outputs.append(output)\n",
    "\n",
    "    outputs = np.array(outputs)\n",
    "    filename = \"_\".join(offer.split(\"/\"))\n",
    "    with open(f'./images_embeded/{filename}.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'image_size': L,\n",
    "            'offer_id': offer,\n",
    "            'outputs': outputs,\n",
    "        }, f)\n",
    "    print(f\"Saved to ./images_embeded/{filename}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_size': (672, 672),\n",
       " 'offer_id': '30805/3685/OMS',\n",
       " 'outputs': array([[ 2.560301  , -0.8504364 , -0.21759939, ...,  1.52431   ,\n",
       "          1.125218  ,  1.6827465 ],\n",
       "        [ 1.9245269 , -0.48709422,  1.9455004 , ..., -0.94634795,\n",
       "          0.5189094 ,  0.967928  ],\n",
       "        [ 1.4143376 , -1.6160402 ,  0.3519848 , ...,  2.9981334 ,\n",
       "          1.7953353 ,  2.053301  ],\n",
       "        ...,\n",
       "        [-0.58763826, -0.72419184, -0.693054  , ..., -1.3873552 ,\n",
       "          1.4864492 , -2.251434  ],\n",
       "        [ 1.7955742 , -1.2687701 , -0.7398259 , ..., -0.5495759 ,\n",
       "          1.591101  ,  0.20677093],\n",
       "        [ 1.497448  , -0.86951196,  0.15145272, ..., -0.02245994,\n",
       "          1.4777577 ,  1.5281867 ]], dtype=float32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./images_embeded/30805_3685_OMS.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real-estate-dinov2-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
